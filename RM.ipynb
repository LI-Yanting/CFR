{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regret Matching Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RPS:\n",
    "    n_actions = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    # Actions: 0:Rock  1:Paper  2:Scissors\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.strategy = np.zeros(RPS.n_actions)\n",
    "        self.avg_strategy = np.zeros(RPS.n_actions)\n",
    "        self.strategy_sum = np.zeros(RPS.n_actions)\n",
    "        self.regret_sum = np.zeros(RPS.n_actions)\n",
    "    \n",
    "    def getStrategy(self):\n",
    "        '''Get current mixed strategy through regret-matching'''\n",
    "        self.strategy = np.copy(self.regret_sum)\n",
    "        self.strategy[self.strategy < 0] = 0 # reset negative regrets to zero\n",
    "        normalizing_sum = np.sum(self.strategy)\n",
    "#         print(\"Strategy: {}\".format(self.strategy))\n",
    "#         print(\"normalizing_sum: {}\".format(normalizing_sum))\n",
    "        if(normalizing_sum > 0):\n",
    "            self.strategy /= normalizing_sum\n",
    "        else:\n",
    "            self.strategy = np.repeat(1 / RPS.n_actions, RPS.n_actions)\n",
    "        self.strategy_sum += self.strategy\n",
    "#         print(\"Strategy: {}\".format(self.strategy))\n",
    "    \n",
    "    def getAverageStrategy(self):\n",
    "        '''Get average mixed strategy across all training iterations'''\n",
    "        normalizing_sum = np.sum(self.strategy_sum)\n",
    "        if(normalizing_sum > 0):\n",
    "            self.avg_strategy = self.strategy_sum/normalizing_sum\n",
    "        else:\n",
    "            self.avg_strategy = np.repeat(1 / RPS.n_actions, RPS.n_actions)\n",
    "        \n",
    "    def getAction(self):\n",
    "        '''Get random action according to mixed-strategy distribution'''\n",
    "        r = random()\n",
    "        a = 0\n",
    "        cumulative_proba = 0\n",
    "#         print(\"Random number r: {}\".format(r))\n",
    "        while(a < RPS.n_actions-1):\n",
    "            cumulative_proba += self.strategy[a]\n",
    "            if(cumulative_proba > r):\n",
    "                break\n",
    "            a += 1\n",
    "        return a\n",
    "    \n",
    "    def regret(self, my_action, opp_action):\n",
    "        action_utility = np.zeros(RPS.n_actions)\n",
    "        action_utility[opp_action] = 0\n",
    "        action_utility[0 if(opp_action == RPS.n_actions-1) else opp_action+1] = 1\n",
    "        action_utility[RPS.n_actions-1 if(opp_action == 0) else opp_action-1] = -1\n",
    "        regret = action_utility - action_utility[my_action]\n",
    "        self.regret_sum += regret\n",
    "#         print(\"Regret_Sum: {}\".format(self.regret_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 = Player(\"Nicho\")\n",
    "# print(p1.getAction())\n",
    "# p1.getStrategy()\n",
    "# p1.regret(2,0)\n",
    "# print(p1.regret_sum)\n",
    "# p1.getStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Two_Player_Game:\n",
    "    def __init__(self, max_iter=10000):\n",
    "        self.p1 = Player(\"Player1\")\n",
    "        self.p2 = Player(\"Player2\")\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def train(self, avg_regret_matching=False):\n",
    "        # Because here we implement the 2-player version, so we modified the architecture from the paper as below\n",
    "        # To note the times each player wins\n",
    "        num_wins = {\n",
    "            self.p1: 0,\n",
    "            self.p2: 0,\n",
    "            'Draw': 0\n",
    "        }\n",
    "        for i in range(self.max_iter):\n",
    "            # Step 1: Get regret-matching mixed-strategy actions\n",
    "            if avg_regret_matching:\n",
    "                self.p1.getAverageStrategy()\n",
    "                self.p2.getAverageStrategy()\n",
    "            else:\n",
    "                self.p1.getStrategy()\n",
    "                self.p2.getStrategy()\n",
    "            a1 = self.p1.getAction()\n",
    "            a2 = self.p2.getAction()\n",
    "            # Step 2&3: Compute action utilities\n",
    "            # regret function contains the accumulate action regret in Player\n",
    "            self.p1.regret(a1,a2)\n",
    "            self.p2.regret(a2,a1)\n",
    "\n",
    "            winner = self.winner(a1, a2)\n",
    "            num_wins[winner] += 1\n",
    "        \n",
    "        print(\"Statistics over {} runs:\\n\".format(self.max_iter))\n",
    "        print(\"Player1 wins {} times\".format(num_wins[self.p1]))\n",
    "        print(\"Player2 wins {} times\".format(num_wins[self.p2]))\n",
    "        print(\"Draw {} times\".format(num_wins['Draw']))\n",
    "\n",
    "    def winner(self, a1, a2):\n",
    "        if (a1 == a2): return 'Draw'\n",
    "        elif (a1-a2 ==1 or a2-a1 ==2): return self.p1\n",
    "        else: return self.p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Two_Player_Game(max_iter=10000)\n",
    "print(\"==== With simple regret-matching strategy === \")\n",
    "t0 = time()\n",
    "game.train()\n",
    "print(\"----------------------------------------------\\n Done in {0:.3} s\".format(time()-t0))\n",
    "\n",
    "print(\"==== With averaged regret-matching strategy === \")\n",
    "t0 = time()\n",
    "game.train(avg_regret_matching=True)\n",
    "print(\"----------------------------------------------\\n Done in {0:.3} s\".format(time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda_py36]",
   "language": "python",
   "name": "conda-env-conda_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
