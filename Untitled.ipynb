{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual Regret Minimization\n",
    "\n",
    "reference:\n",
    "- on Kuhn Poker: \n",
    "\n",
    "https://zhuanlan.zhihu.com/p/30438383\n",
    "\n",
    "https://github.com/tansey/pycfr\n",
    "- on Scissors-Paper-Rock(SPR):\n",
    "\n",
    "http://www.atyun.com/7659_%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E7%9A%84%E7%8C%9C%E6%8B%B3%EF%BC%9A%E5%8F%8D%E4%BA%8B%E5%AE%9E%E9%81%97%E6%86%BE%E6%9C%80%E5%B0%8F%E5%8C%96%E7%AE%97%E6%B3%95.html\n",
    "\n",
    "https://hackernoon.com/artificial-intelligence-poker-and-regret-part-1-36c78d955720\n",
    "\n",
    "- Regret Mininization example: (pg11, pg14)\n",
    "\n",
    "https://ocw.mit.edu/courses/sloan-school-of-management/15-s50-poker-theory-and-analytics-january-iap-2015/lecture-notes/MIT15_S50IAP15_L7_GameTheor.pdf\n",
    "\n",
    "- explanation:\n",
    "\n",
    "https://www.quora.com/What-is-an-intuitive-explanation-of-counterfactual-regret-minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KuhnTrainer:\n",
    "    def __init__(self, max_iter=1000000):\n",
    "        '''Contains Kuhn Poker definitions'''\n",
    "        self.max_iter = max_iter\n",
    "        self.p1 = Player(\"Player1\")\n",
    "        self.p2 = Player(\"Player2\")\n",
    "        self.n_actions = 2\n",
    "        self.outer_var = 1\n",
    "        self.node = self.Node()\n",
    "    \n",
    "    # About inner class in python: https://stackoverflow.com/questions/1765677/nested-classes-scope\n",
    "    \n",
    "    class Node:\n",
    "        '''Information set node class definition'''\n",
    "        # Kuhn node definitions\n",
    "        def __init__(self):\n",
    "            self.regret_sum = np.zeros(self.n_actions)\n",
    "            self.strategy = np.zeros(self.n_actions)\n",
    "            self.strategy_sum = np.zeros(self.n_actions)\n",
    "            \n",
    "        # Get current information set mixed strategy through regret-matching\n",
    "        def getStrategy(self):\n",
    "#             self.strategy = np.copy(self.regret_sum[player][infoset])\n",
    "            self.strategy = np.copy(self.regret_sum)\n",
    "            self.strategy[self.strategy < 0] = 0 # reset negative regrets to zero\n",
    "            normalizing_sum = np.sum(self.strategy)\n",
    "            if(normalizing_sum > 0):\n",
    "                self.strategy /= normalizing_sum\n",
    "            else:\n",
    "                self.strategy = np.repeat(1 / RPS.n_actions, RPS.n_actions)\n",
    "            self.strategy_sum += self.strategy\n",
    "            \n",
    "        # Get average information set mixed strategy across all training iteration\n",
    "        # Get information set string representation\n",
    "        \n",
    "        def getAverageStrategy(self):\n",
    "            \n",
    "    \n",
    "    def train(self):\n",
    "        '''Train Kuhn poker'''\n",
    "        cards = [1,2,3]\n",
    "        util = 0\n",
    "        for i in range(self.max_iter):\n",
    "            # Shuffle cards\n",
    "            for c1 in range(len(cards)-1, -1, -1):\n",
    "                c2 = random.randint(0,c1+1)\n",
    "                tmp = cards[c1]\n",
    "                cards[c1] = cards[c2]\n",
    "                cards[c2] = tmp\n",
    "                \n",
    "            util += cfr(cards, \"\", 1,1)\n",
    "        print(\"Average game value: {}\".format(util.self.max_iter))\n",
    "        \n",
    "        ## print node\n",
    "        \n",
    "    def cfr(self):\n",
    "        '''Counterfactual regret minimization iteration'''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "test = KuhnTrainer(max_iter=10)\n",
    "for i in range(10):\n",
    "    print(random.randint(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = KuhnTrainer(max_iter = 100)\n",
    "trainer.train()\n",
    "trainer.checkNash()\n",
    "print(\"Player one avg strategy:\")\n",
    "trainer.playerOneTree.PrintAvgStrategy()\n",
    "print(\"Player one best resp strategy:\")\n",
    "trainer.playerOneTree.PrintBestResp()\n",
    "print(\"----------------------\")\n",
    "print(\"Player two avg strategy:\")\n",
    "trainer.playerTwoTree.PrintAvgStrategy()\n",
    "print(\"Player two best resp strategy:\")\n",
    "trainer.playerTwoTree.PrintBestResp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Creating a game tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a strategy profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the best response strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a Nash equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CFR(action_history, reach_probs, player_cards): \n",
    "    if isTerminal(action_history): #是不是到了叶子节点\n",
    "        current_player = len(action_history)%2 \n",
    "    return getUtility(action_history, player_cards, current_player)\n",
    "\n",
    "    counterfactual_value_h = 0.0\n",
    "    counterfactual_value_h_a = [0.0, 0.0]\n",
    "    current_player = len(action_history)%2 #根据当前action history 计算infomation set\n",
    "    info_set = str(player_cards[current_player])\n",
    "    for i in range(len(action_history)):\n",
    "        info_set += str(action_history[i])\n",
    "        \n",
    "    \n",
    "    for i in range(2): #两种可能的行动 pass 0; bet 1;\n",
    "        new_action_history = action_history\n",
    "        new_action_history.append(i)\n",
    "        new_reach_probs = reach_probs\n",
    "        new_reach_probs[current_player] = new_reach_probs[current_player]*cur_strategy[info_set+str(i)] \n",
    "        counterfactual_value_h_a[i] = CFR(new_action_history,new_reach_probs, player_cards)\n",
    "        counterfactual_value_h += cur_strategy[info_set+str(i)]*counterfactual_value_h_a[i]\n",
    " \n",
    "   regretSum = 0.0 for i in range(2):\n",
    "        regret[info_set+str(i)] += reach_probs[1-current_player]*(counterfactual_value_h_a[i] - counterfactual_value_h)\n",
    "        AccStrategy[info_set+str(i)] += reach_probs[current_player]*cur_strategy[info_set+str(i)]\n",
    "        if regret[info_set+str(i)] > 0:\n",
    "            regretSum += regret[info_set+str(i)]\n",
    "    for i in range(2):\n",
    "        if regretSum > 0:\n",
    "            cur_strategy[info_set+str(i)] =  max(0, regret[info_set+str(i)])/regretSum\n",
    "        else:\n",
    "            cur_strategy[info_set+str(i)] = 0.5\n",
    "        strategySum[info_set+str(i)]+= cur_strategy[info_set+str(i)]\n",
    "    \n",
    "    return counterfactual_value_h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda_py36]",
   "language": "python",
   "name": "conda-env-conda_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
